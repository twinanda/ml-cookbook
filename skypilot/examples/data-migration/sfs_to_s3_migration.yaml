resources:
  cloud: nebius
  region: eu-west1 # https://docs.nebius.com/overview/regions
  cpus: 16 # https://docs.nebius.com/compute/virtual-machines/types#cpu-configurations

num_nodes: 2 # amount of nodes to parallel download
workdir: .

envs:
  # =================================================
  # Required variables - must be set before launching
  # =================================================
  SRC_DIR: /mnt/sfs
  DST_BUCKET: s3://test
  DST_BUCKET_REGION: <region>
  DST_BUCKET_ENDPOINT_URL: https://storage.<region>.nebius.cloud
  DST_BUCKET_ACCESS_KEY_ID: <access-key-id>
  DST_BUCKET_ACCESS_KEY: <access-key>
  # =================================================

setup: |
  echo 'Installing required tools...'

  # Install latest rclone
  curl https://rclone.org/install.sh | sudo bash

  # Install other utilities
  sudo apt-get update -qq > /dev/null 2>&1
  sudo apt-get install -y jq bc -qq > /dev/null 2>&1
  
  # Create rclone config directory
  mkdir -p ~/.config/rclone

  # Generate rclone config file
  echo "[s3]" > ~/.config/rclone/rclone.conf
  echo "type = s3" >> ~/.config/rclone/rclone.conf
  echo "provider = AWS" >> ~/.config/rclone/rclone.conf
  echo "env_auth = false" >> ~/.config/rclone/rclone.conf
  echo "region = ${DST_BUCKET_REGION}" >> ~/.config/rclone/rclone.conf
  echo "no_check_bucket = true" >> ~/.config/rclone/rclone.conf
  echo "endpoint = ${DST_BUCKET_ENDPOINT_URL}" >> ~/.config/rclone/rclone.conf
  echo "acl = private" >> ~/.config/rclone/rclone.conf
  echo "bucket_acl = private" >> ~/.config/rclone/rclone.conf
  echo "access_key_id = ${DST_BUCKET_ACCESS_KEY_ID}" >> ~/.config/rclone/rclone.conf
  echo "secret_access_key = ${DST_BUCKET_ACCESS_KEY}" >> ~/.config/rclone/rclone.conf

  echo "Rclone configuration file created successfully"

run: |
  # Configuration and environment setup
  export TASK_ID=${SKYPILOT_TASK_ID}
  export NODE_RANK=${SKYPILOT_NODE_RANK}
  export NUM_NODES=${SKYPILOT_NUM_NODES}
  # Export environment variables
  export SRC_DIR=${SRC_DIR}
  export DST_BUCKET=${DST_BUCKET}
  export DST_BUCKET_REGION=${DST_BUCKET_REGION}
  export DST_BUCKET_ENDPOINT_URL=${DST_BUCKET_ENDPOINT_URL}
  export DST_BUCKET_ACCESS_KEY_ID=${DST_BUCKET_ACCESS_KEY_ID}
  export DST_BUCKET_ACCESS_KEY=${DST_BUCKET_ACCESS_KEY}

  export TEMP_DIR="$SRC_DIR/s3migration"
  sudo mkdir -p $TEMP_DIR
  sudo chmod 777 $TEMP_DIR

  # Verify directory creation and accessibility
  if [ ! -d "$TEMP_DIR" ]; then
    echo "Error: Failed to create temporary directory $TEMP_DIR"
    exit 1
  fi

  if [ ! -w "$TEMP_DIR" ]; then
    echo "Error: No write permission for temporary directory $TEMP_DIR"
    exit 1
  fi

  echo "Temporary directory $TEMP_DIR is available and writable"

  echo "Starting SFS to S3 migration task on node ${NODE_RANK}..."
  
  # Record start time
  START_TIME=$(date +%s)

  echo "Starting SFS to S3 migration task: ${TASK_ID}"
  echo "Running on node ${NODE_RANK} of ${NUM_NODES} nodes"
  echo "Migration started at $(date)"

  # Head node lists all objects and distributes work
  if [ "${NODE_RANK}" = "0" ]; then
    echo "Head node: Listing all objects in source bucket"
    
    # List all objects
    rclone lsf $SRC_DIR --files-only --format sp --recursive | grep -v ";s3migration/" > $TEMP_DIR/all_objects.txt
    
    # Count total objects and calculate estimated size
    TOTAL_OBJECTS=$(wc -l < $TEMP_DIR/all_objects.txt)
    TOTAL_SIZE_BYTES=$(awk '{sum += $1} END {print sum}' $TEMP_DIR/all_objects.txt)
    TOTAL_SIZE_GB=$(echo "scale=2; $TOTAL_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
    
    echo "Found $TOTAL_OBJECTS objects with total size of approximately $TOTAL_SIZE_GB GB"
    
    # Distribute objects evenly among nodes
    echo "Distributing objects among $NUM_NODES nodes"
    
    # Split files by node using modulo on line number
    for i in $(seq 0 $((NUM_NODES-1))); do
      awk -v node=$i -v nodes=$NUM_NODES 'NR % nodes == node' $TEMP_DIR/all_objects.txt > $TEMP_DIR/node_${i}_objects.txt
      
      # Count files and total size for this node
      NODE_OBJECTS=$(wc -l < $TEMP_DIR/node_${i}_objects.txt)
      NODE_SIZE_BYTES=$(awk '{sum += $1} END {print sum}' $TEMP_DIR/node_${i}_objects.txt)
      NODE_SIZE_GB=$(echo "scale=2; $NODE_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
      
      echo "Node $i assigned $NODE_OBJECTS objects with total size of approximately $NODE_SIZE_GB GB"
      
    done
    echo "Object distribution complete."
  fi
  # end head node

  # Wait for 5 seconds to ensure files are uploaded
  sleep 5

  # Each node processes its assigned files
  echo "Processing node ${NODE_RANK}"

  cut -d';' -f2 $TEMP_DIR/node_${NODE_RANK}_objects.txt > $TEMP_DIR/node_${NODE_RANK}_list.txt
  rclone copy --files-from $TEMP_DIR/node_${NODE_RANK}_list.txt $SRC_DIR $DST_BUCKET \
  --progress --links --disable-http2 --transfers=128
  echo "done" >$TEMP_DIR/node_${NODE_RANK}_done

  # Head node verifies all nodes have completed
  if [ "${NODE_RANK}" = "0" ]; then
    echo "Head node: Waiting for all worker nodes to complete"
    
    # Wait for all workers to complete
    for worker in $(seq 1 $((NUM_NODES-1))); do
      echo "Waiting for worker $worker to complete..."
      while true; do
        if [ -f "$TEMP_DIR/node_${worker}_done" ]; then
          echo "Worker $worker has completed"
          break
        fi
        sleep 2
      done
    done
    
    echo "All workers have completed. Verifying object counts..."
    
    # Count objects in source and target buckets
    # List source bucket files
    rclone lsf $SRC_DIR --recursive --files-only > $TEMP_DIR/final_source_objects.txt
    rclone lsf $DST_BUCKET --recursive --files-only > $TEMP_DIR/final_target_files.txt
    
    # Filter out directory entries and temp files
    TEMP_DIR_NAME=$(basename "$TEMP_DIR")
    grep -v "^${TEMP_DIR_NAME}/" "$TEMP_DIR/final_source_objects.txt" > "$TEMP_DIR/final_source_files.txt"
    
    # Extract just the object keys for comparison
    cat $TEMP_DIR/final_source_files.txt | awk '{print $NF}' | sort > $TEMP_DIR/source_keys.txt
    cat $TEMP_DIR/final_target_files.txt | awk '{print $NF}' | sort > $TEMP_DIR/target_keys.txt
    
    # Compare counts
    SOURCE_COUNT=$(wc -l < $TEMP_DIR/final_source_files.txt)
    TARGET_COUNT=$(wc -l < $TEMP_DIR/final_target_files.txt)
    
    # Calculate and print migration duration
    END_TIME=$(date +%s)
    DURATION=$((END_TIME - START_TIME))
    HOURS=$((DURATION / 3600))
    MINUTES=$(( (DURATION % 3600) / 60 ))
    SECONDS=$((DURATION % 60))
    
    echo "------------------------------------------------------"
    echo "üìä Migration Statistics:"
    echo "Total migration time: ${HOURS}h ${MINUTES}m ${SECONDS}s"
    echo "Source filesystem: $SOURCE_COUNT objects"
    echo "Target bucket: $TARGET_COUNT objects"
    echo "------------------------------------------------------"
    
    if [ $SOURCE_COUNT -eq $TARGET_COUNT ]; then
      echo "‚úÖ Migration completed successfully! Object counts match."
      
      # Even if counts match, check for differences in file lists
      if diff $TEMP_DIR/source_keys.txt $TEMP_DIR/target_keys.txt > $TEMP_DIR/diff_output.txt; then
        echo "‚úÖ All objects match between source and target."
      else
        echo "‚ö†Ô∏è Warning: Although counts match, some objects differ."
        echo "Objects in source but not in target (first 10):"
        grep "^<" $TEMP_DIR/diff_output.txt | head -10
        echo "Objects in target but not in source (first 10):"
        grep "^>" $TEMP_DIR/diff_output.txt | head -10
      fi
    else
      echo "‚ö†Ô∏è Migration completed with warnings. Object counts don't match."
      echo "Source: $SOURCE_COUNT, Target: $TARGET_COUNT"
      
      # Show detailed differences
      echo "Analyzing differences between source and target ..."
      
      # Find files in source but not in target
      comm -23 $TEMP_DIR/source_keys.txt $TEMP_DIR/target_keys.txt > $TEMP_DIR/missing_in_target.txt
      MISSING_TARGET=$(wc -l < $TEMP_DIR/missing_in_target.txt)

      echo "$MISSING_TARGET files exist in source but not in target"
      
      if [ $MISSING_TARGET -gt 0 ]; then
        echo "Sample of missing files in target (first 10):"
        head -10 $TEMP_DIR/missing_in_target.txt
      fi
      
    fi
    
    # Clean up temporary dir
    #rm -rf $TEMP_DIR
      
  fi
  # end